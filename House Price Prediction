import os
import numpy as np
import pandas as pd
import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
train=pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')
test=pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')
train.head(4)
test.head(4)
train.shape,test.shape
test.isna().sum()
train.info()
null_var=train.isna().sum()/train.shape[0]*100
null_var
drop_columns=null_var[null_var>17].keys()
drop_columns
train_df=train.drop(columns=drop_columns)
train_df.shape
test_df=test.drop(columns=drop_columns)
sns.heatmap(train_df.isna())
import sys

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")
sns.distplot(train_df['MSSubClass'])
sns.regplot(x = 'GrLivArea', y = 'SalePrice', color = 'green', data = train)
plt.show()
train_df.select_dtypes(include=['int64','float64']).columns
num_var=['Id', 'MSSubClass', 'LotArea', 'OverallQual', 'OverallCond',
       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',
       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',
       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',
       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',
       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',
       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',
       'MoSold', 'YrSold', 'SalePrice']
plt.figure(figsize=(25,25))
for i,var in enumerate(num_var):
  plt.subplot(10,4,i+1)
  sns.distplot(train[var],bins=20)
  sns.distplot(train_df[var],bins=20)
plt.show()
cat_var=train_df.select_dtypes(include=['O']).columns
from sklearn.impute import SimpleImputer
imputer=SimpleImputer(strategy='mean')
imputer.fit(train_df[num_var])
SimpleImputer()
imputer.statistics_
imputer.fit_transform(train_df[num_var])
train_df[num_var]=imputer.transform(train_df[num_var])
train_df[num_var].isnull().sum()
cat_imputer=SimpleImputer(strategy='most_frequent')
train_df[cat_var]=cat_imputer.fit_transform(train_df[cat_var])
train_df[cat_var].isna().sum()
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
for var in cat_var:
    train_df[var]=le.fit_transform(train_df[var])
train_df.isna().sum()
plt.figure(figsize=(9,9))
sns.heatmap(train_df.corr(),cmap="seismic")
train_df.info()
x=train_df.iloc[:,:-1]
x.shape
y=train_df['SalePrice']
y.shape
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=.2,random_state=42)
rgr=LinearRegression()
rgr.fit(xtrain,ytrain)
rgr.score(xtest,ytest)
import xgboost as xgb
import catboost as cat
from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
gbk = LinearRegression()
gbk.fit(xtrain, ytrain)
gbk.score(xtest,ytest)
from sklearn.ensemble import GradientBoostingRegressor

gbk = GradientBoostingRegressor()
gbk.fit(xtrain, ytrain)
gbk.score(xtest,ytest)
xgb_model = xgb.XGBRegressor(
   learning_rate=0.1,
                       n_estimators=1000,
                       max_depth=11,
                       min_child_weight=0,
                       gamma=0.7,
                       subsample=1,
                       colsample_bytree=0.7,
                       nthread=-1,
                       scale_pos_weight=1,
                       seed=27,
                       random_state=42)
xgb_model.fit(xtrain,ytrain)
xgb_model.score(xtest,ytest)
cat_model = cat.CatBoostRegressor(
  iterations=350,
        loss_function="MAPE",
        verbose=0,
        grow_policy='SymmetricTree',
        learning_rate=0.1,
        colsample_bylevel=0.8,
        max_depth=11,
        l2_leaf_reg=0.2,
        subsample=.9,
        max_bin=4096,)
cat_model.fit(xtrain,ytrain)
cat_model.score(xtest,ytest)
from lightgbm import LGBMRegressor
lgbm = LGBMRegressor(objective='regression', 
                       num_leaves=6,
                       learning_rate=0.01, 
                       n_estimators=6000,
                       max_bin=200, 
                       bagging_fraction=0.8,
                       bagging_freq=4, 
                       bagging_seed=8,
                       feature_fraction=0.2,
                       feature_fraction_seed=8,
                       min_sum_hessian_in_leaf = 11,
                       verbose=-1,
                       random_state=42)
lgbm.fit(xtrain,ytrain)
lgbm.score(xtest,ytest)
